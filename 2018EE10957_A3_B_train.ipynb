{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjwfx8T5OAJ"
   },
   "source": [
    "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
    "\n",
    "Give editor access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spAvH1fF0Rhg",
    "outputId": "03fd1ef6-84df-4609-ea4e-64022145e89f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-21 20:16:08--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Resolving www.cse.iitd.ac.in (www.cse.iitd.ac.in)... 103.27.9.152\n",
      "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
      "--2021-10-21 20:16:09--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 217313 (212K) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>] 212.22K   115KB/s    in 1.8s    \n",
      "\n",
      "2021-10-21 20:16:12 (115 KB/s) - ‘data.zip’ saved [217313/217313]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## DONT CHANGE THIS CELL\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
    "!unzip data.zip\n",
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "from pdb import set_trace\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jyW7u0t4TVxj"
   },
   "outputs": [],
   "source": [
    "## Various utility functions\n",
    "class args_class:\n",
    "    dataset = \"data/\"\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    gpu = 0\n",
    "    batch_size = 8\n",
    "    results_dir = \"2018EE10957_B_model\"\n",
    "    epochs = 3\n",
    "\n",
    "def set_seed(seed=4):\n",
    "\t# fixing seeds to reproduce results (exact!)\n",
    "\t# torch.backends.cudnn.deterministic = True\n",
    "\ttorch.manual_seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "def parse_args():\n",
    "\treturn args_class()\n",
    "\n",
    "def get_device(gpu_no):\n",
    "\tif torch.cuda.is_available():\n",
    "\t\ttorch.cuda.set_device(gpu_no)\n",
    "\t\treturn torch.device('cuda:{}'.format(gpu_no))\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "14EjR4tmz2x5"
   },
   "outputs": [],
   "source": [
    "def process_data(path, train=True, test=False):\n",
    "    if test == True:\n",
    "        x_test = pandas.read_csv(\"test.data.txt\", delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "        word1 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "        word2 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "        x_test = x_test.drop(\"position\", axis=1)\n",
    "        x_test.loc[:, \"POS\"] = x_test.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "        x_test.insert(3, \"word1\", word1)\n",
    "        x_test.insert(5, \"word2\", word2)\n",
    "        # x_test.to_csv(\"./test.csv\", header=None, index=False)\n",
    "        return x_test\n",
    "    \n",
    "    train = \"train\" if train else \"validation\"\n",
    "    x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
    "    dataset = pandas.concat((x_train, y_train), axis=1)\n",
    "    word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    dataset = dataset.drop(\"position\", axis=1)\n",
    "    dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
    "    dataset.insert(3, \"word1\", word1)\n",
    "    dataset.insert(5, \"word2\", word2)\n",
    "    # dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
    "    return dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_len=128, train=True, test=False, model_name='bert-base-uncased'):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_len = max_len\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "\n",
    "        # convert raw data into pandas data frame\n",
    "        self.dataset_df = process_data(data_dir, train=train, test=test)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) # , do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset_df.iloc[index]\n",
    "\n",
    "        x = self.tokenizer(sample['sen1'], sample['sen2'], return_tensors='pt', add_special_tokens=True, \\\n",
    "            max_length=self.max_len, padding='max_length', truncation=True)\n",
    "        if self.test == False:\n",
    "            y = torch.tensor(sample['label'])\n",
    "\n",
    "        # format as required by Train\n",
    "        item = {k: v[0] for k,v in x.items()} # only one datapoint; shape: 128\n",
    "        if self.test == False:\n",
    "            item['labels'] = y\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/shubham/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/shubham/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/shubham/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/shubham/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/shubham/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/shubham/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/shubham/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource preparation done: 2021-10-25 03:13:34.631836\n"
     ]
    }
   ],
   "source": [
    "set_seed()\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics_fn(eval_prediction):\n",
    "    logits, labels = eval_prediction\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "args = parse_args()\n",
    "trainset = myDataset(args.dataset, max_len=100, train=True, model_name=args.model_name)\n",
    "devset = myDataset(args.dataset, max_len=100, train=False, model_name=args.model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=2)\n",
    "print(\"resource preparation done: {}\".format(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 5428\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2037\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2037' max='2037' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2037/2037 06:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.605600</td>\n",
       "      <td>0.707967</td>\n",
       "      <td>0.639498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.868928</td>\n",
       "      <td>0.681818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.174100</td>\n",
       "      <td>1.495763</td>\n",
       "      <td>0.688088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 638\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to 2018EE10957_B_model/checkpoint-679\n",
      "Configuration saved in 2018EE10957_B_model/checkpoint-679/config.json\n",
      "Model weights saved in 2018EE10957_B_model/checkpoint-679/pytorch_model.bin\n",
      "Deleting older checkpoint [2018EE10957_B_model/checkpoint-2037] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 638\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to 2018EE10957_B_model/checkpoint-1358\n",
      "Configuration saved in 2018EE10957_B_model/checkpoint-1358/config.json\n",
      "Model weights saved in 2018EE10957_B_model/checkpoint-1358/pytorch_model.bin\n",
      "Deleting older checkpoint [2018EE10957_B_model/checkpoint-679] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 638\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to 2018EE10957_B_model/checkpoint-2037\n",
      "Configuration saved in 2018EE10957_B_model/checkpoint-2037/config.json\n",
      "Model weights saved in 2018EE10957_B_model/checkpoint-2037/pytorch_model.bin\n",
      "Deleting older checkpoint [2018EE10957_B_model/checkpoint-1358] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from 2018EE10957_B_model/checkpoint-2037 (score: 0.6880877742946708).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2037, training_loss=0.39342883432619585, metrics={'train_runtime': 363.2283, 'train_samples_per_second': 44.831, 'train_steps_per_second': 5.608, 'total_flos': 836816489352000.0, 'train_loss': 0.39342883432619585, 'epoch': 3.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=args.results_dir, \n",
    "    overwrite_output_dir=True, \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=args.epochs,\n",
    "    seed=4,\n",
    "    per_device_eval_batch_size=args.batch_size,\n",
    "    per_device_train_batch_size=args.batch_size,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=trainset, eval_dataset=devset,\n",
    "    compute_metrics=compute_metrics_fn\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2021-10-25 03:14:06--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.data.txt\n",
      "Connecting to 10.10.78.22:3128... connected.\n",
      "Proxy request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.data.txt [following]\n",
      "--2021-10-25 03:14:06--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.data.txt\n",
      "Connecting to 10.10.78.22:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 63639 (62K) [text/plain]\n",
      "Saving to: ‘test.data.txt’\n",
      "\n",
      "test.data.txt       100%[===================>]  62.15K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2021-10-25 03:14:06 (12.2 MB/s) - ‘test.data.txt’ saved [63639/63639]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## DONT CHANGE THIS CELL \n",
    "# this is currently the same as dev.data.txt\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/shubham/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/shubham/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/shubham/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testset = myDataset(args.dataset, max_len=100, train=False, test=True, model_name=args.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file 2018EE10957_B_model/checkpoint-2037/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file 2018EE10957_B_model/checkpoint-2037/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at 2018EE10957_B_model/checkpoint-2037/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"2018EE10957_B_model/checkpoint-2037/\", num_labels=2)\n",
    "trainer = Trainer(\n",
    "    model=model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions written in output.txt\n"
     ]
    }
   ],
   "source": [
    "out = trainer.predict(testset)\n",
    "predictions = np.argmax(out[0], axis=-1)\n",
    "predictions = pandas.DataFrame(predictions)[0].apply(lambda label : \"T\" if label == 1 else \"F\")\n",
    "predictions.to_csv(\"output.txt\", header=None, index=None)\n",
    "print(\"predictions written in output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "--2021-10-25 03:27:23--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.gold.txt\n",
      "Connecting to 10.10.78.22:3128... connected.\n",
      "Proxy request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.gold.txt [following]\n",
      "--2021-10-25 03:27:23--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.gold.txt\n",
      "Connecting to 10.10.78.22:3128... connected.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 1276 (1.2K) [text/plain]\n",
      "Saving to: ‘test.gold.txt.1’\n",
      "\n",
      "test.gold.txt.1     100%[===================>]   1.25K  --.-KB/s    in 0s      \n",
      "\n",
      "2021-10-25 03:27:23 (23.6 MB/s) - ‘test.gold.txt.1’ saved [1276/1276]\n",
      "\n",
      "Performance =  0.6880877742946708\n"
     ]
    }
   ],
   "source": [
    "## DONT CHANGE THIS CELL\n",
    "# Your testing code must produce a file output.txt with predictions as T and F in each line\n",
    "\n",
    "## Final Evaluation \n",
    "# this is currently the same as dev.gold.txt\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.gold.txt\n",
    "correct, total = 0., 0.\n",
    "for prediction, gold in zip(open('output.txt'), open('test.gold.txt')):\n",
    "    prediction, gold = prediction.strip(), gold.strip()\n",
    "    total += 1\n",
    "    if prediction == gold:\n",
    "        correct += 1\n",
    "\n",
    "## Report this as the final validation performance \n",
    "print('Performance = ', (correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zip the final model and all the required files, such as vocabulary\n",
    "# Replace USERID with your own, such as 2017CSZ8058\n",
    "!zip -r 2018EE10957_A_model.zip 2018EE10957_B_model\n",
    "\n",
    "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset_df = process_data(\"data\", train=False)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "# dataset_df.iloc[12:14]['sen1'].values\n",
    "# sample = dataset_df.iloc[12:14]\n",
    "# x = tokenizer(list(sample['sen1']), list(sample['sen2']), return_tensors='pt', \\\n",
    "#                         add_special_tokens=True, max_length=128, padding='max_length')\n",
    "# y = torch.tensor(list(sample['label']))\n",
    "# for k,v in x.items():\n",
    "#     print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from datasets import load_metric\n",
    "\n",
    "# metric = load_metric(\"accuracy\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# transformers.logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=0.6710249917548999, metrics={'train_runtime': 48.3189, 'train_samples_per_second': 112.337, 'train_steps_per_second': 2.359, 'total_flos': 278938829784000.0, 'train_loss': 0.6710249917548999, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=task.args.results_dir, \n",
    "#     overwrite_output_dir=True, \n",
    "#     evaluation_strategy=\"epoch\", \n",
    "#     learning_rate=5e-5,\n",
    "#     num_train_epochs=3, \n",
    "#     seed=4,\n",
    "#     per_device_eval_batch_size=task.args.batch_size,\n",
    "#     per_device_train_batch_size=task.args.batch_size,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=task.model, args=training_args,\n",
    "#     train_dataset=task.trainset, eval_dataset=task.devset,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# print(tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id, tokenizer.mask_token_id)\n",
    "# tokenizer.encode([\"hey my name is shubham\", \"they 'll play this game very hastily\"], add_special_tokens=True)\n",
    "# print(tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[SEP]\", \"[CLS]\", \"[MASK]\"]))\n",
    "# print(tokenizer.convert_ids_to_tokens([0,100,101, 102, 103, 104]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = process_data(\"./data/\", train=False)\n",
    "# df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = tokenizer([\"heyhey myhey\",\"hi\",\"hiello\"], add_special_tokens=False, return_tensors='pt', padding='max_length', max_length=5, truncation=True)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(out['input_ids'][0])\n",
    "# print(tokens)\n",
    "# sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "# print(sen)\n",
    "# print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# bert = bert.to('cuda')\n",
    "# trainset = WiC_dataset(\"data\", max_len=128, train=True, tokenizer='bert-base-uncased')\n",
    "# trainloader = DataLoader(trainset, batch_size = 32, shuffle = True)\n",
    "# out = bert(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = process_data(\"./data/\", train=True)\n",
    "# max_num_tok = 0\n",
    "# for i in range(len(df)):\n",
    "#     t = tokenizer(df.loc[i,'sen1'], df.loc[i,'sen2'], add_special_tokens=True, return_tensors='pt')['input_ids']\n",
    "#     max_num_tok = max(max_num_tok, t.shape[-1])\n",
    "# max_num_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] an invasion of bees. [SEP] an invasion of mobile phones. [SEP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(t[0])\n",
    "# print(tokens)\n",
    "# sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "# print(sen)\n",
    "# print(\" \".join(tokens))\n",
    "# tokenizer.decode(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def process_data(path, train=True):\n",
    "#     train = \"train\" if train else \"validation\"\n",
    "#     x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "#     y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
    "#     dataset = pandas.concat((x_train, y_train), axis=1)\n",
    "#     word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "#     word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "#     dataset = dataset.drop(\"position\", axis=1)\n",
    "#     dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "#     dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
    "#     dataset.insert(3, \"word1\", word1)\n",
    "#     dataset.insert(5, \"word2\", word2)\n",
    "#     # dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
    "#     return dataset\n",
    "\n",
    "# class myDataset(Dataset):\n",
    "#     def __init__(self, data_dir, max_len=128, train=True, model_name='bert-base-uncased'):\n",
    "#         self.data_dir = data_dir\n",
    "#         self.max_len = max_len\n",
    "#         self.train = train\n",
    "\n",
    "#         # convert raw data into pandas data frame\n",
    "#         self.dataset_df = process_data(data_dir, train=train)\n",
    "\n",
    "#         # tokenizer\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         sample = self.dataset_df.iloc[index]\n",
    "\n",
    "#         x = self.tokenizer(sample['sen1'], sample['sen2'], return_tensors='pt', add_special_tokens=True, \\\n",
    "#             max_length=self.max_len, padding='max_length', truncation=True)\n",
    "#         y = torch.tensor(sample['label'])\n",
    "\n",
    "#         # format as required by Train\n",
    "#         item = {k: v[0] for k,v in x.items()} # only one datapoint; shape: 128\n",
    "#         item['labels'] = y\n",
    "#         return item\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModel\n",
    "# bert = AutoModel.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "# bert = bert.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e2a419e950445eaf549000c5683518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6a0e0bc0d94c718a6fb2f74719eccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd5fb286ef14a04acd18c3c5ddbe475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# trainset = myDataset(\"data\", max_len=100, train=True, model_name=\"bert-base-cased\")\n",
    "# devset = myDataset(\"data\", max_len=100, train=False, model_name=\"bert-base-cased\")\n",
    "\n",
    "# trainloader = DataLoader(trainset, batch_size = 8, shuffle = True)\n",
    "# devloader = DataLoader(devset, batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch=next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp={k:v.to(torch.device('cuda')) for k,v in batch.items() if k != 'labels'}\n",
    "# oup = bert(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 torch.Size([8, 100, 768]) torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "# print(len(oup), oup[0].shape, oup[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for p in bert.parameters():\n",
    "#     p.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJjx9_YFhRzA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAf77kcrhS_1",
    "outputId": "3801bc2e-18ef-4050-d6b7-fb94cadaf78c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.18808777429467\n",
      "predictions written in output.txt\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3ccWcHDhXfV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2018EE10957_A3_B_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
