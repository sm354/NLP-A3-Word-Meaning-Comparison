{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjwfx8T5OAJ"
   },
   "source": [
    "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
    "\n",
    "Give editor access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spAvH1fF0Rhg",
    "outputId": "03fd1ef6-84df-4609-ea4e-64022145e89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-21 20:16:08--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Resolving www.cse.iitd.ac.in (www.cse.iitd.ac.in)... 103.27.9.152\n",
      "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
      "--2021-10-21 20:16:09--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 217313 (212K) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>] 212.22K   115KB/s    in 1.8s    \n",
      "\n",
      "2021-10-21 20:16:12 (115 KB/s) - ‘data.zip’ saved [217313/217313]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## DONT CHANGE THIS CELL\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yARdz5vOTSgW",
    "outputId": "9c0f9a8d-50b4-47cc-908d-6de336462535",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "   creating: data/train/\n",
      "  inflating: data/train/train.gold.txt  \n",
      "  inflating: data/train/train.data.txt  \n",
      "   creating: data/validation/\n",
      "  inflating: data/validation/validation.data.txt  \n",
      "  inflating: data/validation/validation.gold.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LrnkLN2LzlDB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import spacy\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from pdb import set_trace\n",
    "import torch\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "jyW7u0t4TVxj"
   },
   "outputs": [],
   "source": [
    "## Various utility functions\n",
    "class args_class:\n",
    "    dataset = \"data/\"\n",
    "    model_name = \"bert-base-uncased\"\n",
    "    gpu = 0\n",
    "    batch_size = 8\n",
    "    results_dir = \"2018EE10957_B_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "sr3ok6g51O_d"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=4):\n",
    "\t# fixing seeds to reproduce results (exact!)\n",
    "\ttorch.manual_seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "def parse_args():\n",
    "\tparser = ArgumentParser(description='NLP A3-A')\n",
    "\tparser.add_argument('--dataset', '-d', type=str, default='data')\n",
    "\tparser.add_argument('--model_name', '-m', type=str, default='bert-base-uncased')\n",
    "\tparser.add_argument('--gpu', type=int, default=0)\n",
    "\tparser.add_argument('--batch_size', type=int, default=8)\n",
    "\t# parser.add_argument('--hidden_dim', type=int, default=128)\n",
    "\t# parser.add_argument('--epochs', type=int, default=20)\n",
    "\t# parser.add_argument('--lr', type=float, default=0.001)\n",
    "\tparser.add_argument('--results_dir', type=str, default='2018EE10957_B_model')\n",
    "\treturn parser.parse_args()\n",
    "\n",
    "def get_device(gpu_no):\n",
    "\tif torch.cuda.is_available():\n",
    "\t\ttorch.cuda.set_device(gpu_no)\n",
    "\t\treturn torch.device('cuda:{}'.format(gpu_no))\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')\n",
    "\n",
    "def makedirs(name):\n",
    "\t\"\"\"helper function for python 2 and 3 to call os.makedirs()\n",
    "\t\tavoiding an error if the directory to be created already exists\"\"\"\n",
    "\n",
    "\timport os, errno\n",
    "\n",
    "\ttry:\n",
    "\t\tos.makedirs(name)\n",
    "\texcept OSError as ex:\n",
    "\t\tif ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "\t\t\t# ignore existing directory\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\t# a different error happened\n",
    "\t\t\traise\n",
    "\n",
    "def check_folder(log_dir):\n",
    "\tif not os.path.exists(log_dir):\n",
    "\t\tos.makedirs(log_dir)\n",
    "\treturn log_dir\n",
    "\n",
    "def get_logger(args, phase):\n",
    "\tlogging.basicConfig(level=logging.INFO, \n",
    "\t\tfilename = \"{}_{}.log\".format(args.model, phase),\n",
    "\t\tformat = '%(asctime)s - %(message)s', \n",
    "\t\tdatefmt='%d-%b-%y %H:%M:%S'\n",
    "\t)\n",
    "\treturn logging.getLogger(phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "14EjR4tmz2x5"
   },
   "outputs": [],
   "source": [
    "def process_data(path, train=True):\n",
    "    train = \"train\" if train else \"validation\"\n",
    "    x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
    "    dataset = pandas.concat((x_train, y_train), axis=1)\n",
    "    word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    dataset = dataset.drop(\"position\", axis=1)\n",
    "    dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
    "    dataset.insert(3, \"word1\", word1)\n",
    "    dataset.insert(5, \"word2\", word2)\n",
    "    # dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
    "    return dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_len=128, train=True, model_name='bert-base-uncased'):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_len = max_len\n",
    "        self.train = train\n",
    "\n",
    "        # convert raw data into pandas data frame\n",
    "        self.dataset_df = process_data(data_dir, train=train)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset_df.iloc[index]\n",
    "\n",
    "        x = self.tokenizer(sample['sen1'], sample['sen2'], return_tensors='pt', add_special_tokens=True, \\\n",
    "            max_length=self.max_len, padding='max_length', truncation=True)\n",
    "        y = torch.tensor(sample['label'])\n",
    "\n",
    "        # format as required by Train\n",
    "        item = {k: v[0] for k,v in x.items()} # only one datapoint; shape: 128\n",
    "        item['labels'] = y\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset_df = process_data(\"data\", train=False)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "# dataset_df.iloc[12:14]['sen1'].values\n",
    "# sample = dataset_df.iloc[12:14]\n",
    "# x = tokenizer(list(sample['sen1']), list(sample['sen2']), return_tensors='pt', \\\n",
    "#                         add_special_tokens=True, max_length=128, padding='max_length')\n",
    "# y = torch.tensor(list(sample['label']))\n",
    "# for k,v in x.items():\n",
    "#     print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()\n",
    "\n",
    "class Train():\n",
    "    def __init__(self):\n",
    "        print(\"program execution start: {}\".format(datetime.datetime.now()))\n",
    "        self.args = args_class()\n",
    "        self.device = get_device(self.args.gpu)\n",
    "        # self.logger = get_logger(self.args, \"train\")\n",
    "        # self.logger.info(\"Arguments: {}\".format(self.args))\n",
    "\n",
    "        self.trainset = myDataset(self.args.dataset, max_len=100, train=True, model_name=self.args.model_name)\n",
    "        self.devset = myDataset(self.args.dataset, max_len=100, train=False, model_name=self.args.model_name)\n",
    "\n",
    "        # self.trainloader = DataLoader(trainset, batch_size = args.batch_size, shuffle = True)\n",
    "        # self.devloader = DataLoader(devset, batch_size = args.batch_size, shuffle = False)\n",
    "\n",
    "        # self.model = myModel(self.device)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(self.args.model_name, num_labels=2)\n",
    "\n",
    "        # self.model = self.model.to(self.device)\n",
    "        # self.criterion = nn.BCELoss()\n",
    "        # self.opt = O.Adam(self.model.parameters(), lr = self.args.lr) #, weight_decay=0.005)\n",
    "        # self.best_val_acc = None\n",
    "        # self.scheduler = StepLR(self.opt, step_size=5, gamma=0.5)\n",
    "\n",
    "        print(\"resource preparation done: {}\".format(datetime.datetime.now()))\n",
    "\n",
    "    def execute(self):\n",
    "        # some parameters taken from https://huggingface.co/transformers/training.html\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir= self.args.results_dir, \n",
    "            overwrite_output_dir=True, \n",
    "            evaluation_strategy=\"epoch\", \n",
    "            learning_rate=5e-5,\n",
    "            weight_decay=0.01,\n",
    "            num_train_epochs=3, \n",
    "            seed=4,\n",
    "            per_device_eval_batch_size=self.args.batch_size,\n",
    "            per_device_train_batch_size=self.args.batch_size,\n",
    "            warmup_steps=500,\n",
    "            logging_dir='./logs',\n",
    "            logging_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"eval_accuracy\",\n",
    "            greater_is_better=True,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=self.model, args=training_args,\n",
    "            train_dataset=self.trainset, eval_dataset=self.devset,\n",
    "            compute_metrics=compute_metrics# deplag\n",
    "        )\n",
    "\n",
    "        trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from datasets import load_metric\n",
    "\n",
    "# metric = load_metric(\"accuracy\")\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "#     logits, labels = eval_pred\n",
    "#     predictions = np.argmax(logits, axis=-1)\n",
    "#     return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import transformers\n",
    "# transformers.logging.set_verbosity(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lrl1BDMaTy5T",
    "outputId": "6011a234-b1a2-40b4-e3e5-26617e920619",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program execution start: 2021-10-24 14:27:32.931952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resource preparation done: 2021-10-24 14:27:55.225020\n"
     ]
    }
   ],
   "source": [
    "## Start training\n",
    "task = Train()\n",
    "task.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 5428\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 456\n",
      "/home/shubham/anaconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='456' max='456' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [456/456 02:20, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.095100</td>\n",
       "      <td>1.146554</td>\n",
       "      <td>0.677116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.046500</td>\n",
       "      <td>1.725659</td>\n",
       "      <td>0.666144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>2.072562</td>\n",
       "      <td>0.655172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>1.395411</td>\n",
       "      <td>0.670846</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 638\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to 2018EE10957_B_model/checkpoint-114\n",
      "Configuration saved in 2018EE10957_B_model/checkpoint-114/config.json\n",
      "Model weights saved in 2018EE10957_B_model/checkpoint-114/pytorch_model.bin\n",
      "/home/shubham/anaconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 638\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to 2018EE10957_B_model/checkpoint-228\n",
      "Configuration saved in 2018EE10957_B_model/checkpoint-228/config.json\n",
      "Model weights saved in 2018EE10957_B_model/checkpoint-228/pytorch_model.bin\n",
      "/home/shubham/anaconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 638\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to 2018EE10957_B_model/checkpoint-342\n",
      "Configuration saved in 2018EE10957_B_model/checkpoint-342/config.json\n",
      "Model weights saved in 2018EE10957_B_model/checkpoint-342/pytorch_model.bin\n",
      "Deleting older checkpoint [2018EE10957_B_model/checkpoint-228] due to args.save_total_limit\n",
      "/home/shubham/anaconda3/envs/nlp/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 638\n",
      "  Batch size = 48\n",
      "Saving model checkpoint to 2018EE10957_B_model/checkpoint-456\n",
      "Configuration saved in 2018EE10957_B_model/checkpoint-456/config.json\n",
      "Model weights saved in 2018EE10957_B_model/checkpoint-456/pytorch_model.bin\n",
      "Deleting older checkpoint [2018EE10957_B_model/checkpoint-342] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from 2018EE10957_B_model/checkpoint-114 (score: 0.677115987460815).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir= task.args.results_dir, \n",
    "    overwrite_output_dir=True, \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=4, \n",
    "    seed=4,\n",
    "    per_device_eval_batch_size=task.args.batch_size,\n",
    "    per_device_train_batch_size=task.args.batch_size,\n",
    "    warmup_steps=500,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=task.model, args=training_args,\n",
    "    train_dataset=task.trainset, eval_dataset=task.devset,\n",
    "    compute_metrics=compute_metrics# deplag\n",
    ")\n",
    "\n",
    "m=trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=0.6710249917548999, metrics={'train_runtime': 48.3189, 'train_samples_per_second': 112.337, 'train_steps_per_second': 2.359, 'total_flos': 278938829784000.0, 'train_loss': 0.6710249917548999, 'epoch': 1.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zip the final model and all the required files, such as vocabulary\n",
    "# Replace USERID with your own, such as 2017CSZ8058\n",
    "!zip -r 2018EE10957_A_model.zip 2018EE10957_B_model\n",
    "\n",
    "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=task.args.results_dir, \n",
    "#     overwrite_output_dir=True, \n",
    "#     evaluation_strategy=\"epoch\", \n",
    "#     learning_rate=5e-5,\n",
    "#     num_train_epochs=3, \n",
    "#     seed=4,\n",
    "#     per_device_eval_batch_size=task.args.batch_size,\n",
    "#     per_device_train_batch_size=task.args.batch_size,\n",
    "# )\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     model=task.model, args=training_args,\n",
    "#     train_dataset=task.trainset, eval_dataset=task.devset,\n",
    "# )\n",
    "\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# print(tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id, tokenizer.mask_token_id)\n",
    "# tokenizer.encode([\"hey my name is shubham\", \"they 'll play this game very hastily\"], add_special_tokens=True)\n",
    "# print(tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[SEP]\", \"[CLS]\", \"[MASK]\"]))\n",
    "# print(tokenizer.convert_ids_to_tokens([0,100,101, 102, 103, 104]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = process_data(\"./data/\", train=False)\n",
    "# df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = tokenizer([\"heyhey myhey\",\"hi\",\"hiello\"], add_special_tokens=False, return_tensors='pt', padding='max_length', max_length=5, truncation=True)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(out['input_ids'][0])\n",
    "# print(tokens)\n",
    "# sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "# print(sen)\n",
    "# print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "# bert = bert.to('cuda')\n",
    "# trainset = WiC_dataset(\"data\", max_len=128, train=True, tokenizer='bert-base-uncased')\n",
    "# trainloader = DataLoader(trainset, batch_size = 32, shuffle = True)\n",
    "# out = bert(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df = process_data(\"./data/\", train=True)\n",
    "# max_num_tok = 0\n",
    "# for i in range(len(df)):\n",
    "#     t = tokenizer(df.loc[i,'sen1'], df.loc[i,'sen2'], add_special_tokens=True, return_tensors='pt')['input_ids']\n",
    "#     max_num_tok = max(max_num_tok, t.shape[-1])\n",
    "# max_num_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] an invasion of bees. [SEP] an invasion of mobile phones. [SEP]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(t[0])\n",
    "# print(tokens)\n",
    "# sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "# print(sen)\n",
    "# print(\" \".join(tokens))\n",
    "# tokenizer.decode(t[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path, train=True):\n",
    "    train = \"train\" if train else \"validation\"\n",
    "    x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
    "    dataset = pandas.concat((x_train, y_train), axis=1)\n",
    "    word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    dataset = dataset.drop(\"position\", axis=1)\n",
    "    dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
    "    dataset.insert(3, \"word1\", word1)\n",
    "    dataset.insert(5, \"word2\", word2)\n",
    "    # dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
    "    return dataset\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, data_dir, max_len=128, train=True, model_name='bert-base-uncased'):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_len = max_len\n",
    "        self.train = train\n",
    "\n",
    "        # convert raw data into pandas data frame\n",
    "        self.dataset_df = process_data(data_dir, train=train)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset_df.iloc[index]\n",
    "\n",
    "        x = self.tokenizer(sample['sen1'], sample['sen2'], return_tensors='pt', add_special_tokens=True, \\\n",
    "            max_length=self.max_len, padding='max_length', truncation=True)\n",
    "        y = torch.tensor(sample['label'])\n",
    "\n",
    "        # format as required by Train\n",
    "        item = {k: v[0] for k,v in x.items()} # only one datapoint; shape: 128\n",
    "        item['labels'] = y\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "bert = AutoModel.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
    "bert = bert.to(torch.device('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58e2a419e950445eaf549000c5683518",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a6a0e0bc0d94c718a6fb2f74719eccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd5fb286ef14a04acd18c3c5ddbe475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainset = myDataset(\"data\", max_len=100, train=True, model_name=\"bert-base-cased\")\n",
    "devset = myDataset(\"data\", max_len=100, train=False, model_name=\"bert-base-cased\")\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size = 8, shuffle = True)\n",
    "devloader = DataLoader(devset, batch_size = 8, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=next(iter(trainloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp={k:v.to(torch.device('cuda')) for k,v in batch.items() if k != 'labels'}\n",
    "oup = bert(**inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 torch.Size([8, 100, 768]) torch.Size([8, 768])\n"
     ]
    }
   ],
   "source": [
    "print(len(oup), oup[0].shape, oup[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in bert.parameters():\n",
    "    p.requires_grad=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mn6wE-LU_Bc"
   },
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJjx9_YFhRzA"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    hidden_dim=64\n",
    "    device = get_device(0)\n",
    "\n",
    "    x_test = pandas.read_csv(\"test.data.txt\", delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    word1 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    x_test = x_test.drop(\"position\", axis=1)\n",
    "    x_test.loc[:, \"POS\"] = x_test.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    x_test.insert(3, \"word1\", word1)\n",
    "    x_test.insert(5, \"word2\", word2)\n",
    "    x_test.to_csv(\"./test.csv\", header=None, index=False)\n",
    "\n",
    "    with open(os.path.join(\"2018EE10957_A_model\", \"Field_TEXT\"), 'rb') as f:\n",
    "        TEXT = dill.load(f)\n",
    "\n",
    "    fields = [\n",
    "        ('word', TEXT),\n",
    "        ('POS', data.Field(use_vocab=False, sequential=False)), \n",
    "        ('sen1', TEXT),\n",
    "        ('word1', data.Field(use_vocab=False, sequential=False)),\n",
    "        ('sen2', TEXT),\n",
    "        ('word2', data.Field(use_vocab=False, sequential=False)),\n",
    "    ]\n",
    "\n",
    "    test_set = data.TabularDataset(\n",
    "        path = \"./test.csv\",\n",
    "        format = 'csv',\n",
    "        fields = fields,\n",
    "        skip_header = False\n",
    "    )\n",
    "\n",
    "    test_itr = data.BucketIterator(\n",
    "        test_set,\n",
    "        #sort_key = lambda sample : len(sample.sen1),\n",
    "        sort = False,\n",
    "        batch_size = 32,\n",
    "        repeat = False,\n",
    "        shuffle = False,\n",
    "    )\n",
    "\n",
    "    model = myModel(pretrained_embeddings=TEXT.vocab.vectors, embed_dim=TEXT.vocab.vectors.shape[1], hidden_dim=hidden_dim, device=device)\n",
    "    saved_model = torch.load(\"2018EE10957_A_model/best-biLSTM-params.pt\", map_location=device)\n",
    "    print(saved_model[\"accuracy\"])\n",
    "    model.load_state_dict(saved_model['model_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    test_itr.init_epoch()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_itr):\n",
    "            answer = model(batch)\n",
    "            answer = list(np.array((answer>=0.5).long().cpu()))\n",
    "            predictions += answer\n",
    "\n",
    "    predictions = pandas.DataFrame(predictions)[0].apply(lambda label : \"T\" if label == 1 else \"F\")\n",
    "    predictions.to_csv(\"output.txt\", header=None, index=None)\n",
    "    print(\"predictions written in output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAf77kcrhS_1",
    "outputId": "3801bc2e-18ef-4050-d6b7-fb94cadaf78c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.18808777429467\n",
      "predictions written in output.txt\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3ccWcHDhXfV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2018EE10957_A3_B_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
