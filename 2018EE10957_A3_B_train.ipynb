{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjwfx8T5OAJ"
   },
   "source": [
    "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
    "\n",
    "Give editor access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spAvH1fF0Rhg",
    "outputId": "03fd1ef6-84df-4609-ea4e-64022145e89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-21 20:16:08--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Resolving www.cse.iitd.ac.in (www.cse.iitd.ac.in)... 103.27.9.152\n",
      "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
      "--2021-10-21 20:16:09--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 217313 (212K) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>] 212.22K   115KB/s    in 1.8s    \n",
      "\n",
      "2021-10-21 20:16:12 (115 KB/s) - ‘data.zip’ saved [217313/217313]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## DONT CHANGE THIS CELL\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yARdz5vOTSgW",
    "outputId": "9c0f9a8d-50b4-47cc-908d-6de336462535",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "   creating: data/train/\n",
      "  inflating: data/train/train.gold.txt  \n",
      "  inflating: data/train/train.data.txt  \n",
      "   creating: data/validation/\n",
      "  inflating: data/validation/validation.data.txt  \n",
      "  inflating: data/validation/validation.gold.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LrnkLN2LzlDB"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "import time\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from pdb import set_trace\n",
    "import torch\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "# from torchtext.legacy import data\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "jyW7u0t4TVxj"
   },
   "outputs": [],
   "source": [
    "## Various utility functions\n",
    "class args_class:\n",
    "    dataset = \"data/\"\n",
    "    model = \"biLSTM\"\n",
    "    gpu = 0\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    hidden_dim = 64\n",
    "    lr = 0.001\n",
    "    results_dir = \"2018EE10957_B_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "sr3ok6g51O_d"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=4):\n",
    "\t# fixing seeds to reproduce results (exact!)\n",
    "\ttorch.manual_seed(seed)\n",
    "\trandom.seed(seed)\n",
    "\tnp.random.seed(seed)\n",
    "\n",
    "def parse_args():\n",
    "# \tparser = ArgumentParser(description='NLP A3-A')\n",
    "# \tparser.add_argument('--dataset', '-d', type=str, default='data')\n",
    "# \tparser.add_argument('--model', '-m', type=str, default='biLSTM')\n",
    "# \tparser.add_argument('--gpu', type=int, default=0)\n",
    "# \tparser.add_argument('--batch_size', type=int, default=32)\n",
    "# \tparser.add_argument('--hidden_dim', type=int, default=128)\n",
    "# \tparser.add_argument('--epochs', type=int, default=20)\n",
    "# \tparser.add_argument('--lr', type=float, default=0.001)\n",
    "# \tparser.add_argument('--results_dir', type=str, default='2018EE10957_A_model')\n",
    "\treturn args_class()\n",
    "\n",
    "\"\"\"checking arguments\"\"\"\n",
    "def check_args(args):\n",
    "\t# --result_dir\n",
    "\tcheck_folder(os.path.join(args.dataset))\n",
    "\tcheck_folder(os.path.join(args.results_dir))\n",
    "\n",
    "\t# --epoch\n",
    "\ttry:\n",
    "\t\t\tassert args.epochs >= 1\n",
    "\texcept:\n",
    "\t\t\tprint('number of epochs must be larger than or equal to one')\n",
    "\n",
    "\t# --batch_size\n",
    "\ttry:\n",
    "\t\t\tassert args.batch_size >= 1\n",
    "\texcept:\n",
    "\t\t\tprint('batch size must be larger than or equal to one')\n",
    "\treturn args\n",
    "\n",
    "def get_device(gpu_no):\n",
    "\tif torch.cuda.is_available():\n",
    "\t\ttorch.cuda.set_device(gpu_no)\n",
    "\t\treturn torch.device('cuda:{}'.format(gpu_no))\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')\n",
    "\n",
    "def makedirs(name):\n",
    "\t\"\"\"helper function for python 2 and 3 to call os.makedirs()\n",
    "\t\tavoiding an error if the directory to be created already exists\"\"\"\n",
    "\n",
    "\timport os, errno\n",
    "\n",
    "\ttry:\n",
    "\t\tos.makedirs(name)\n",
    "\texcept OSError as ex:\n",
    "\t\tif ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "\t\t\t# ignore existing directory\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\t# a different error happened\n",
    "\t\t\traise\n",
    "\n",
    "def check_folder(log_dir):\n",
    "\tif not os.path.exists(log_dir):\n",
    "\t\tos.makedirs(log_dir)\n",
    "\treturn log_dir\n",
    "\n",
    "def get_logger(args, phase):\n",
    "\tlogging.basicConfig(level=logging.INFO, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tfilename = \"{}_{}.log\".format(args.model, phase),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tformat = '%(asctime)s - %(message)s', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdatefmt='%d-%b-%y %H:%M:%S')\n",
    "\treturn logging.getLogger(phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "14EjR4tmz2x5"
   },
   "outputs": [],
   "source": [
    "def process_data(path, train=True):\n",
    "    train = \"train\" if train else \"validation\"\n",
    "    x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
    "    dataset = pandas.concat((x_train, y_train), axis=1)\n",
    "    word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    dataset = dataset.drop(\"position\", axis=1)\n",
    "    dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
    "    dataset.insert(3, \"word1\", word1)\n",
    "    dataset.insert(5, \"word2\", word2)\n",
    "    dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
    "    return dataset\n",
    "\n",
    "class WiC_dataset(Dataset):\n",
    "    def __init__(self, data_dir, max_len=128, train=True, tokenizer='bert-base-uncased'):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_len = max_len\n",
    "        self.train = train\n",
    "\n",
    "        # convert raw data into pandas data frame\n",
    "        self.dataset_df = process_data(data_dir, train=train)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset_df.iloc[index]\n",
    "\n",
    "        x = self.tokenizer(sample['sen1'], sample['sen2'], return_tensors='pt', \\\n",
    "                                add_special_tokens=True, max_length=self.max_len, padding='max_length')\n",
    "        y = torch.tensor(sample['label'])\n",
    "\n",
    "        item = {}\n",
    "        for k,v in x.items():\n",
    "            item[k] = v[0] # only one datapoint; shape: 128\n",
    "        item['labels'] = y\n",
    "\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# dataset_df = process_data(\"data\", train=False)\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_df.iloc[12:14]['sen1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = dataset_df.iloc[12:14]\n",
    "# x = tokenizer(list(sample['sen1']), list(sample['sen2']), return_tensors='pt', \\\n",
    "#                         add_special_tokens=True, max_length=128, padding='max_length')\n",
    "# y = torch.tensor(list(sample['label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k,v in x.items():\n",
    "#     print(v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "q383CZB3TzI4"
   },
   "outputs": [],
   "source": [
    "# class myModel(nn.Module):\n",
    "#     def __init__(self, model_name=\"bert-base-uncased\", device=torch.device('cpu')):\n",
    "#         super(myModel, self).__init__()\n",
    "#         # self.embed_dim = embed_dim\n",
    "#         # self.hidden_dim = hidden_dim\n",
    "#         self.device = device\n",
    "#         self.bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2) #, output_attentions=False, output_hidden_states=False)\n",
    "        \n",
    "#         # self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=True)\n",
    "#         # self.dropout = nn.Dropout(p = 0.5)\n",
    "#         # self.bilstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=False, batch_first=True, num_layers=2)\n",
    "#         # self.fc = nn.Linear(hidden_dim, 2)\n",
    "#         # self.attn = nn.MultiheadAttention(embed_dim=300, num_heads=4, batch_first=True) #self.hidden_dim\n",
    "#         # self.similarity = nn.CosineSimilarity(dim=1)\n",
    "#         # self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "#     def forward(self, input):\n",
    "#         out = self.bert(input)\n",
    "#         return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/shubham/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/shubham/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/shubham/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/shubham/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/shubham/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/shubham/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "device = get_device(0)\n",
    "\n",
    "trainset = WiC_dataset(args.dataset, max_len=128, train=True, tokenizer='bert-base-uncased')\n",
    "devset = WiC_dataset(args.dataset, max_len=128, train=False, tokenizer='bert-base-uncased')\n",
    "\n",
    "# trainloader = DataLoader(trainset, batch_size = args.batch_size, shuffle = True)\n",
    "# devloader = DataLoader(devset, batch_size = args.batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x,y = next(iter(trainset))\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"dir\", overwrite_output_dir=True, \n",
    "    evaluation_strategy=\"epoch\", learning_rate=5e-5,\n",
    "    num_train_epochs=3, seed=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, args=training_args,\n",
    "    train_dataset=trainset, eval_dataset=devset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/shubham/.cache/huggingface/datasets/imdb/plain_text/1.0.0/e3c66f1788a67a89c7058d97ff62b6c30531e05b549de56d3ab91891f0561f9a)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f51661bf5bbf4abfadeabd60418c1c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /home/shubham/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /home/shubham/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/shubham/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /home/shubham/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# raw_datasets['test']['text']\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bfb671e44e64c13b3fb15492c13d1a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f514ad6cb1ff4952a686222338f5b357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630c3d68726e405f86df7c1459a069e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "full_train_dataset = tokenized_datasets[\"train\"]\n",
    "full_eval_dataset = tokenized_datasets[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'text', 'token_type_ids'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lrl1BDMaTy5T",
    "outputId": "6011a234-b1a2-40b4-e3e5-26617e920619",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program execution start: 2021-10-23 03:02:41.674205\n",
      "TEXT Field saved in 2018EE10957_B_model\n",
      "resource preparation done: 2021-10-23 03:02:46.979472\n",
      " [*] Training starts!\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Epoch  20 | train loss  0.01 | train acc 91.71 | val loss  0.02 | val acc 58.78 | time:  1.79s |: 100%|█| 20/20 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      " [*] Training finished!\n",
      "best validation accuracy: 63.47962382445141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Start training\n",
    "task = Train()\n",
    "task.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "# print(tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id, tokenizer.mask_token_id)\n",
    "# tokenizer.encode([\"hey my name is shubham\", \"they 'll play this game very hastily\"], add_special_tokens=True)\n",
    "# print(tokenizer.convert_tokens_to_ids([\"[PAD]\", \"[SEP]\", \"[CLS]\", \"[MASK]\"]))\n",
    "# print(tokenizer.convert_ids_to_tokens([0,100,101, 102, 103, 104]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "word                                     board\n",
       "POS                                          1\n",
       "sen1                          Room and board .\n",
       "word1                                        2\n",
       "sen2     He nailed boards across the windows .\n",
       "word2                                        2\n",
       "label                                        0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = process_data(\"./data/\", train=False)\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = tokenizer([\"heyhey myhey\",\"hi\",\"hiello\"], add_special_tokens=True, return_tensors='pt', padding='max_length', max_length=10) #, truncation=True)\n",
    "# out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokens = tokenizer.convert_ids_to_tokens(out['input_ids'][0])\n",
    "# print(tokens)\n",
    "# sen = tokenizer.convert_tokens_to_string(tokens)\n",
    "# print(sen)\n",
    "# print(\" \".join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "bert = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "bert = bert.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class WiC_dataset(Dataset):\n",
    "    def __init__(self, data_dir, max_len=128, train=True, tokenizer='bert-base-uncased'):\n",
    "        self.data_dir = data_dir\n",
    "        self.max_len = max_len\n",
    "        self.train = train\n",
    "\n",
    "        # convert raw data into pandas data frame\n",
    "        self.dataset_df = process_data(data_dir, train=train)\n",
    "\n",
    "        # tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer, do_lower_case=True)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.dataset_df.iloc[index]\n",
    "\n",
    "        tokenized = self.tokenizer(sample['sen1'], sample['sen2'], return_tensors='pt', \\\n",
    "                                add_special_tokens=True, max_length=self.max_len, padding='max_length')\n",
    "        \n",
    "        x = tokenized['input_ids']\n",
    "        y = torch.tensor(sample['label'])\n",
    "        return x,y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = WiC_dataset(\"data\", max_len=128, train=True, tokenizer='bert-base-uncased')\n",
    "trainloader = DataLoader(trainset, batch_size = 32, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = bert(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XxKdCr2jUCwP",
    "outputId": "f85c0782-8f46-4923-f109-87582436100b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: 2018EE10957_A_model/ (stored 0%)\n",
      "  adding: 2018EE10957_A_model/best-biLSTM-params.pt (deflated 9%)\n",
      "  adding: 2018EE10957_A_model/Field_TEXT (deflated 11%)\n"
     ]
    }
   ],
   "source": [
    "## Zip the final model and all the required files, such as vocabulary\n",
    "# Replace USERID with your own, such as 2017CSZ8058\n",
    "!zip -r 2018EE10957_A_model.zip 2018EE10957_A_model\n",
    "\n",
    "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mn6wE-LU_Bc"
   },
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SJjx9_YFhRzA"
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    hidden_dim=64\n",
    "    device = get_device(0)\n",
    "\n",
    "    x_test = pandas.read_csv(\"test.data.txt\", delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    word1 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    x_test = x_test.drop(\"position\", axis=1)\n",
    "    x_test.loc[:, \"POS\"] = x_test.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    x_test.insert(3, \"word1\", word1)\n",
    "    x_test.insert(5, \"word2\", word2)\n",
    "    x_test.to_csv(\"./test.csv\", header=None, index=False)\n",
    "\n",
    "    with open(os.path.join(\"2018EE10957_A_model\", \"Field_TEXT\"), 'rb') as f:\n",
    "        TEXT = dill.load(f)\n",
    "\n",
    "    fields = [\n",
    "        ('word', TEXT),\n",
    "        ('POS', data.Field(use_vocab=False, sequential=False)), \n",
    "        ('sen1', TEXT),\n",
    "        ('word1', data.Field(use_vocab=False, sequential=False)),\n",
    "        ('sen2', TEXT),\n",
    "        ('word2', data.Field(use_vocab=False, sequential=False)),\n",
    "    ]\n",
    "\n",
    "    test_set = data.TabularDataset(\n",
    "        path = \"./test.csv\",\n",
    "        format = 'csv',\n",
    "        fields = fields,\n",
    "        skip_header = False\n",
    "    )\n",
    "\n",
    "    test_itr = data.BucketIterator(\n",
    "        test_set,\n",
    "        #sort_key = lambda sample : len(sample.sen1),\n",
    "        sort = False,\n",
    "        batch_size = 32,\n",
    "        repeat = False,\n",
    "        shuffle = False,\n",
    "    )\n",
    "\n",
    "    model = myModel(pretrained_embeddings=TEXT.vocab.vectors, embed_dim=TEXT.vocab.vectors.shape[1], hidden_dim=hidden_dim, device=device)\n",
    "    saved_model = torch.load(\"2018EE10957_A_model/best-biLSTM-params.pt\", map_location=device)\n",
    "    print(saved_model[\"accuracy\"])\n",
    "    model.load_state_dict(saved_model['model_dict'])\n",
    "    model = model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    test_itr.init_epoch()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_itr):\n",
    "            answer = model(batch)\n",
    "            answer = list(np.array((answer>=0.5).long().cpu()))\n",
    "            predictions += answer\n",
    "\n",
    "    predictions = pandas.DataFrame(predictions)[0].apply(lambda label : \"T\" if label == 1 else \"F\")\n",
    "    predictions.to_csv(\"output.txt\", header=None, index=None)\n",
    "    print(\"predictions written in output.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAf77kcrhS_1",
    "outputId": "3801bc2e-18ef-4050-d6b7-fb94cadaf78c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.18808777429467\n",
      "predictions written in output.txt\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3ccWcHDhXfV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2018EE10957_A3_B_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
