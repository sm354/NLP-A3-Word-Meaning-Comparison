{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWjwfx8T5OAJ"
   },
   "source": [
    "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
    "\n",
    "Give editor access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spAvH1fF0Rhg",
    "outputId": "f704c41b-804b-475b-9156-3a0cb3e99e02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-10-11 01:19:29--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Connecting to 10.10.78.22:3128... connected.\n",
      "Proxy request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
      "--2021-10-11 01:19:29--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
      "Connecting to 10.10.78.22:3128... connected.\n",
      "WARNING: cannot verify www.cse.iitd.ac.in's certificate, issued by ‘CN=R3,O=Let's Encrypt,C=US’:\n",
      "  Issued certificate has expired.\n",
      "Proxy request sent, awaiting response... 200 OK\n",
      "Length: 217313 (212K) [application/zip]\n",
      "Saving to: ‘data.zip’\n",
      "\n",
      "data.zip            100%[===================>] 212.22K  --.-KB/s    in 0.003s  \n",
      "\n",
      "2021-10-11 01:19:29 (63.7 MB/s) - ‘data.zip’ saved [217313/217313]\n",
      "\n",
      "Archive:  data.zip\n",
      "   creating: data/\n",
      "   creating: data/train/\n",
      "  inflating: data/train/train.gold.txt  \n",
      "  inflating: data/train/train.data.txt  \n",
      "   creating: data/validation/\n",
      "  inflating: data/validation/validation.data.txt  \n",
      "  inflating: data/validation/validation.gold.txt  \n"
     ]
    }
   ],
   "source": [
    "## DONT CHANGE THIS CELL\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip --no-check-certificate\n",
    "!unzip data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "LrnkLN2LzlDB"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import spacy\n",
    "import dill\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from pdb import set_trace\n",
    "import torch\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "UfCesmSwIr9K"
   },
   "outputs": [],
   "source": [
    "## Various utility functions\n",
    "class args_class:\n",
    "    dataset = \"data/\"\n",
    "    model = \"biLSTM\"\n",
    "    gpu = 0\n",
    "    batch_size = 32\n",
    "    epochs = 20\n",
    "    hidden_dim = 128\n",
    "    lr = 0.001\n",
    "    results_dir = \"2018EE10957_A_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "sr3ok6g51O_d"
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "\t# parser = ArgumentParser(description='NLI Baseline')\n",
    "\t# parser.add_argument('--dataset', '-d', type=str, default='mnli')\n",
    "\t# parser.add_argument('--model', '-m', type=str, default='bilstm')\n",
    "\t# parser.add_argument('--gpu', type=int, default=0)\n",
    "\t# parser.add_argument('--batch_size', type=int, default=128)\n",
    "\t# parser.add_argument('--embed_dim', type=int, default=300)\n",
    "\t# parser.add_argument('--d_hidden', type=int, default=200)\n",
    "\t# parser.add_argument('--dp_ratio', type=int, default=0.2)\n",
    "\t# parser.add_argument('--epochs', type=int, default=20)\n",
    "\t# parser.add_argument('--lr', type=float, default=0.001)\n",
    "\t# parser.add_argument('--combine', type=str, default='cat')\n",
    "\t# parser.add_argument('--results_dir', type=str, default='results')\n",
    "\targs = args_class()\n",
    "\treturn check_args(args)\n",
    "\n",
    "\"\"\"checking arguments\"\"\"\n",
    "def check_args(args):\n",
    "\t# --result_dir\n",
    "\tcheck_folder(os.path.join(args.dataset))\n",
    "\tcheck_folder(os.path.join(args.results_dir))\n",
    "\n",
    "\t# --epoch\n",
    "\ttry:\n",
    "\t\t\tassert args.epochs >= 1\n",
    "\texcept:\n",
    "\t\t\tprint('number of epochs must be larger than or equal to one')\n",
    "\n",
    "\t# --batch_size\n",
    "\ttry:\n",
    "\t\t\tassert args.batch_size >= 1\n",
    "\texcept:\n",
    "\t\t\tprint('batch size must be larger than or equal to one')\n",
    "\treturn args\n",
    "\n",
    "def get_device(gpu_no):\n",
    "\tif torch.cuda.is_available():\n",
    "\t\ttorch.cuda.set_device(gpu_no)\n",
    "\t\treturn torch.device('cuda:{}'.format(gpu_no))\n",
    "\telse:\n",
    "\t\treturn torch.device('cpu')\n",
    "\n",
    "def makedirs(name):\n",
    "\t\"\"\"helper function for python 2 and 3 to call os.makedirs()\n",
    "\t\tavoiding an error if the directory to be created already exists\"\"\"\n",
    "\n",
    "\timport os, errno\n",
    "\n",
    "\ttry:\n",
    "\t\tos.makedirs(name)\n",
    "\texcept OSError as ex:\n",
    "\t\tif ex.errno == errno.EEXIST and os.path.isdir(name):\n",
    "\t\t\t# ignore existing directory\n",
    "\t\t\tpass\n",
    "\t\telse:\n",
    "\t\t\t# a different error happened\n",
    "\t\t\traise\n",
    "\n",
    "def check_folder(log_dir):\n",
    "\tif not os.path.exists(log_dir):\n",
    "\t\tos.makedirs(log_dir)\n",
    "\treturn log_dir\n",
    "\n",
    "def get_logger(args, phase):\n",
    "\tlogging.basicConfig(level=logging.INFO, \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tfilename = \"{}_{}.log\".format(args.model, phase),\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tformat = '%(asctime)s - %(message)s', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdatefmt='%d-%b-%y %H:%M:%S')\n",
    "\treturn logging.getLogger(phase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "NfBzKQ06HhZ7"
   },
   "outputs": [],
   "source": [
    "def process_data(path, train=True):\n",
    "    train = \"train\" if train else \"validation\"\n",
    "    x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
    "    dataset = pandas.concat((x_train, y_train), axis=1)\n",
    "    word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    dataset = dataset.drop(\"position\", axis=1)\n",
    "    dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
    "    dataset.insert(3, \"word1\", word1)\n",
    "    dataset.insert(5, \"word2\", word2)\n",
    "    dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
    "\n",
    "en = spacy.load('en_core_web_sm')\n",
    "def tokeni(sen):\n",
    "    t = en.tokenizer(sen)\n",
    "    return [word.text for word in t]\n",
    "\n",
    "class myDataset:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        \n",
    "        process_data(args.dataset, train=True)\n",
    "        process_data(args.dataset, train=False)\n",
    "\n",
    "        TEXT = data.Field(\n",
    "            sequential=True,\n",
    "            lower=True,\n",
    "            tokenize=tokeni, # lambda sen : sen.split(\" \"),\n",
    "        )\n",
    "        # get_tokenizer(\"basic_english\"),\n",
    "        fields = [\n",
    "            ('word', TEXT),\n",
    "            ('POS', data.Field(use_vocab=False, sequential=False)), \n",
    "            ('sen1', TEXT),\n",
    "            ('word1', data.Field(use_vocab=False, sequential=False)),\n",
    "            ('sen2', TEXT),\n",
    "            ('word2', data.Field(use_vocab=False, sequential=False)),\n",
    "            ('label', data.Field(use_vocab=False, sequential=False)),\n",
    "        ]\n",
    "\n",
    "        train_set, val_set = data.TabularDataset.splits(\n",
    "            path = args.dataset,\n",
    "            train = 'train/train.csv',\n",
    "            validation = 'validation/validation.csv',\n",
    "            format = 'csv',\n",
    "            fields = fields,\n",
    "            skip_header = False\n",
    "        )\n",
    "\n",
    "        TEXT.build_vocab(train_set, vectors='glove.6B.300d') # max_size=20000, min_freq=2)\n",
    "        # TEXT.build_vocab(train_set, vectors=args.model)\n",
    "\n",
    "        train_itr, val_itr = data.BucketIterator.splits(\n",
    "            (train_set, val_set),\n",
    "            #sort_key = lambda sample : len(sample.sen1),\n",
    "            sort = False,\n",
    "            batch_size = args.batch_size,\n",
    "            repeat = False\n",
    "        )\n",
    "        with open(os.path.join(self.args.results_dir, \"Field_TEXT\"), 'wb') as f:\n",
    "            dill.dump(TEXT, f)\n",
    "        print(\"TEXT Field saved in %s\"%self.args.results_dir)\n",
    "        \n",
    "        self.vocab = TEXT.vocab\n",
    "        self.train_iter = train_itr\n",
    "        self.dev_iter = val_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kjTtU7mSHhOS"
   },
   "outputs": [],
   "source": [
    "\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, embed_dim=100, hidden_dim=100, device=torch.device('cpu')):\n",
    "        super(myModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=False, batch_first=True, num_layers=1)\n",
    "        self.similarity = nn.CosineSimilarity(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "            [torchtext.legacy.data.batch.Batch of size 32]\n",
    "                [.word]:[torch.LongTensor of size 1x32]\n",
    "                [.POS]:[torch.LongTensor of size 32]\n",
    "                [.sen1]:[torch.LongTensor of size 17x32]\n",
    "                [.word1]:[torch.LongTensor of size 32]\n",
    "                [.sen2]:[torch.LongTensor of size 26x32]\n",
    "                [.word2]:[torch.LongTensor of size 32]\n",
    "                [.label]:[torch.LongTensor of size 32]\n",
    "        '''\n",
    "        input1 = input.sen1.permute(1,0).to(self.device)\n",
    "        input2 = input.sen2.permute(1,0).to(self.device)\n",
    "\n",
    "        # embed the tokens (indices in the vocab) \n",
    "        out1 = self.dropout(self.embedding(input1))\n",
    "        out2 = self.dropout(self.embedding(input2))\n",
    "\n",
    "        out1, (_, _) = self.bilstm(out1)\n",
    "        out2, (_, _) = self.bilstm(out2)\n",
    "\n",
    "        # word of interest\n",
    "        woi1_idx = torch.repeat_interleave(torch.unsqueeze(torch.unsqueeze(input.word1, 1), 2), self.hidden_dim, dim=2).to(self.device)\n",
    "        woi2_idx = torch.repeat_interleave(torch.unsqueeze(torch.unsqueeze(input.word2, 1), 2), self.hidden_dim, dim=2).to(self.device)\n",
    "\n",
    "        # gather the word of interest for each sentence in the batch\n",
    "        out1 = torch.gather(input=out1, dim=1, index=woi1_idx).squeeze()\n",
    "        out2 = torch.gather(input=out2, dim=1, index=woi2_idx).squeeze()\n",
    "        \n",
    "        # compute scores of similarity\n",
    "        out = self.similarity(out1, out2)\n",
    "        out = self.sigmoid(out)\n",
    "        # try:\n",
    "        #     assert out.shape[0] == self.bs\n",
    "        # except:\n",
    "        #     print(\"AssertionError\", out.shape[0], self.bs) # all samples are touched in a epoch\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "14EjR4tmz2x5"
   },
   "outputs": [],
   "source": [
    "# fixing seeds to reproduce results (exact!)\n",
    "torch.manual_seed(4)\n",
    "random.seed(4)\n",
    "np.random.seed(4)\n",
    "\n",
    "class Train():\n",
    "    def __init__(self):\n",
    "        print(\"program execution start: {}\".format(datetime.datetime.now()))\n",
    "        self.args = parse_args()\n",
    "        self.device = get_device(self.args.gpu)\n",
    "        self.logger = get_logger(self.args, \"train\")\n",
    "        self.logger.info(\"Arguments: {}\".format(self.args))\n",
    "\n",
    "        dataset_options = {\n",
    "            'batch_size': self.args.batch_size, \n",
    "            'device': self.device\n",
    "            }\n",
    "\n",
    "        self.dataset = myDataset(self.args)\n",
    "        self.vocab = self.dataset.vocab\n",
    "        self.embed_dim = self.vocab.vectors.shape[1]\n",
    "\n",
    "        self.model = myModel(self.vocab.vectors, self.embed_dim, self.args.hidden_dim, self.device)\n",
    "\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.opt = O.Adam(self.model.parameters(), lr = self.args.lr)\n",
    "        self.best_val_acc = None\n",
    "        # self.scheduler = StepLR(self.opt, step_size=5, gamma=0.5)\n",
    "\n",
    "        print(\"resource preparation done: {}\".format(datetime.datetime.now()))\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train(); self.dataset.train_iter.init_epoch()\n",
    "        n_correct, n_total, n_loss = 0, 0, 0\n",
    "        for batch_idx, batch in enumerate(self.dataset.train_iter):\n",
    "            # if batch.batch_size != self.args.batch_size:\n",
    "            #     print(batch.batch_size)\n",
    "            self.opt.zero_grad()\n",
    "            batch.label = batch.label.to(self.device)\n",
    "            answer = self.model(batch)\n",
    "            loss = self.criterion(answer, batch.label.float())\n",
    "            answer = (answer>=0.5).long()\n",
    "\n",
    "            n_correct += (answer == batch.label).sum().item()\n",
    "            n_total += batch.batch_size\n",
    "            n_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(self.model.parameters(), 5)\n",
    "            self.opt.step()\n",
    "        train_loss = n_loss/n_total\n",
    "        train_acc = 100. * n_correct/n_total\n",
    "        return train_loss, train_acc\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval(); self.dataset.dev_iter.init_epoch()\n",
    "        n_correct, n_total, n_loss = 0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, batch in enumerate(self.dataset.dev_iter):\n",
    "                batch.label = batch.label.to(self.device)\n",
    "                answer = self.model(batch)\n",
    "                loss = self.criterion(answer, batch.label.float())\n",
    "                answer = (answer>=0.5).long()\n",
    "\n",
    "                n_correct += (answer == batch.label).sum().item()\n",
    "                n_total += batch.batch_size\n",
    "                n_loss += loss.item()\n",
    "\n",
    "            # print(np.bincount(answer.cpu()), np.bincount(batch.label.cpu()))\n",
    "            val_loss = n_loss/n_total\n",
    "            val_acc = 100. * n_correct/n_total\n",
    "            return val_loss, val_acc\n",
    "\n",
    "    def result_checkpoint(self, epoch, train_loss, val_loss, train_acc, val_acc, took):\n",
    "        if self.best_val_acc is None or val_acc > self.best_val_acc:\n",
    "            self.best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'accuracy': self.best_val_acc,\n",
    "                'model_dict': self.model.state_dict(),\n",
    "            }, '{}/best-{}-params.pt'.format(self.args.results_dir, self.args.model))\n",
    "        self.logger.info('| Epoch {:3d} | train loss {:5.2f} | train acc {:5.2f} | val loss {:5.2f} | val acc {:5.2f} | time: {:5.2f}s |'\n",
    "                .format(epoch, train_loss, train_acc, val_loss, val_acc, took))\n",
    "\n",
    "    def execute(self):\n",
    "        print(\" [*] Training starts!\")\n",
    "        print('-' * 99)\n",
    "        pbar = tqdm(range(1, self.args.epochs+1))\n",
    "        for epoch in pbar:\n",
    "            start = time.time()\n",
    "\n",
    "            train_loss, train_acc = self.train()\n",
    "            val_loss, val_acc = self.validate()\n",
    "            # self.scheduler.step()\n",
    "\n",
    "            took = time.time()-start\n",
    "            self.result_checkpoint(epoch, train_loss, val_loss, train_acc, val_acc, took)\n",
    "\n",
    "            pbar.set_description('| Epoch {:3d} | train loss {:5.2f} | train acc {:5.2f} | val loss {:5.2f} | val acc {:5.2f} | time: {:5.2f}s |'.format(\n",
    "                epoch, train_loss, train_acc, val_loss, val_acc, took))\n",
    "        self.finish()\n",
    "\n",
    "    def finish(self):\n",
    "        self.logger.info(\"[*] Training finished!\\n\\n\")\n",
    "        print('-' * 99)\n",
    "        print(\" [*] Training finished!\")\n",
    "        print(\"best validation accuracy:\", self.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dM2QDq2iz-h7",
    "outputId": "e3b79b59-a6cc-49c1-fba6-53f40c9bc978"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program execution start: 2021-10-11 01:19:52.699250\n",
      "TEXT Field saved in 2018EE10957_A_model\n",
      "resource preparation done: 2021-10-11 01:19:57.304503\n",
      " [*] Training starts!\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| Epoch  20 | train loss  0.01 | train acc 88.10 | val loss  0.02 | val acc 56.58 | time:  1.37s |: 100%|██████████| 20/20 [00:27<00:00,  1.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      " [*] Training finished!\n",
      "best validation accuracy: 60.9717868338558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Start training\n",
    "task = Train()\n",
    "task.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "G-j7z1e6OjGO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: data/ (stored 0%)\n",
      "  adding: data/validation/ (stored 0%)\n",
      "  adding: data/validation/validation.csv (deflated 59%)\n",
      "  adding: data/validation/validation.data.txt (deflated 58%)\n",
      "  adding: data/validation/validation.gold.txt (deflated 87%)\n",
      "  adding: data/train/ (stored 0%)\n",
      "  adding: data/train/train.data.txt (deflated 63%)\n",
      "  adding: data/train/train.csv (deflated 63%)\n",
      "  adding: data/train/train.gold.txt (deflated 90%)\n"
     ]
    }
   ],
   "source": [
    "## Zip the final model and all the required files, such as vocabulary\n",
    "# Replace USERID with your own, such as 2017CSZ8058\n",
    "!zip -r 2018EE10957_A_model.zip 2018EE10957_A_model\n",
    "\n",
    "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-D5T5iRKnOf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2018EE10957_A3_A_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
