{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "LrnkLN2LzlDB"
   },
   "outputs": [],
   "source": [
    "## Import relevant packages\n",
    "import torch\n",
    "import pandas\n",
    "import spacy\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x_train = pandas.read_csv('./data/train/train.data.txt', delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "# y_train = pandas.read_csv('./data/train/train.gold.txt', header=None, names=['label'])\n",
    "# trainset = pandas.concat((x_train, y_train), axis=1)\n",
    "# print(trainset.POS.unique(), trainset.label.unique())\n",
    "# trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_val = pandas.read_csv('./data/validation/validation.data.txt', delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "# y_val = pandas.read_csv('./data/validation/validation.gold.txt', header=None, names=['label'])\n",
    "# valset = pandas.concat((x_val, y_val), axis=1)\n",
    "# print(valset.POS.unique(), valset.label.unique())\n",
    "# valset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# tokenizer = get_tokenizer('basic_english')\n",
    "# def _build_vocab(corpus):\n",
    "#     for sen in corpus:\n",
    "#         yield tokenize(sen)\n",
    "        \n",
    "# spl_tokens = [\"<UNK>\", \"<SEP>\", \"<PAD>\"]\n",
    "# UNK_id, SEP_id, PAD_id = 0, 1, 2\n",
    "\n",
    "# corpus_iter = trainset.loc[:,['word','sen1','sen2']].values.reshape(-1)\n",
    "# vocab = build_vocab_from_iterator(_build_vocab(corpus_iter), specials=spl_tokens, special_first=True)\n",
    "# vocab.set_default_index(UNK_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # collate function is used when we have map-style dataset like raw data per se\n",
    "# def collate_fn(dataset):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "# traindata_loader = DataLoader(trainset.values, shuffle=True, batchsize=32, collate_fn=collate_fn)\n",
    "# valdata_loader = DataLoader(valset.values, shuffle=False, batchsize=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainset.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# def combine_xy(path, train=True):\n",
    "#     train = 'train' if train else 'validation'\n",
    "#     x = open(os.path.join(path, '%s/%s.data.txt'%(train,train))).readlines()\n",
    "#     y = open(os.path.join(path, '%s/%s.gold.txt'%(train,train))).readlines()\n",
    "#     data = []\n",
    "#     for x_,y_ in zip(x,y):\n",
    "#         data.append(x_[:-1] + \"\\t\" + y_)\n",
    "#     out = open(os.path.join(path, '%s/%s.tsv'%(train,train)), 'w')\n",
    "#     out.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine_xy('data',True)\n",
    "# combine_xy('data',False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = get_tokenizer(\"basic_english\")\n",
    "# print(t(\"dont\"))\n",
    "# import spacy\n",
    "# en = spacy.load('en')\n",
    "# t = en.tokenizer\n",
    "# for x in t(\"shubham mittal\"):\n",
    "#     print(x.text)\n",
    "# for x in t(\"dont\"):\n",
    "#     print(x.text)\n",
    "# for x in t(\"don't\"):\n",
    "#     print(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(path, train=True):\n",
    "    train = \"train\" if train else \"validation\"\n",
    "    x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "    y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
    "    dataset = pandas.concat((x_train, y_train), axis=1)\n",
    "    word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "    word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "    dataset = dataset.drop(\"position\", axis=1)\n",
    "    dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "    dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
    "    dataset.insert(3, \"word1\", word1)\n",
    "    dataset.insert(5, \"word2\", word2)\n",
    "    dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = process_data('./data', train=True)\n",
    "# trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas.read_csv('./data/train/train.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "valset = process_data(\"./data\", train=False)\n",
    "# valset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = get_tokenizer(\"basic_english\")\n",
    "# t(valset.loc[635,'sen2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(\n",
    "    sequential=True,\n",
    "    lower=True,\n",
    "    tokenize=get_tokenizer(\"basic_english\"),\n",
    ")\n",
    "fields = [\n",
    "    ('word', TEXT),\n",
    "    ('POS', data.Field(use_vocab=False, sequential=False)), \n",
    "    ('sen1', TEXT),\n",
    "    ('word1', data.Field(use_vocab=False, sequential=False)),\n",
    "    ('sen2', TEXT),\n",
    "    ('word2', data.Field(use_vocab=False, sequential=False)),\n",
    "    ('label', data.Field(use_vocab=False, sequential=False)),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = data.TabularDataset.splits(\n",
    "    path = './data/',\n",
    "    train = 'train/train.csv',\n",
    "    validation = 'validation/validation.csv',\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = False\n",
    ")\n",
    "# test_set = data.TabularDataset(\n",
    "#     path='',\n",
    "#     fields = fields[:-1]\n",
    "#     skip_header = False,\n",
    "# )\n",
    "TEXT.build_vocab(train_set, vectors='glove.6B.100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['carry'] ['you', 'must', 'carry', 'your', 'camping', 'gear', '.'] ['sound', 'carries', 'well', 'over', 'water', '.']\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0].word, train_set[0].sen1, train_set[0].sen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vars(train_set[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_itr, val_itr = data.BucketIterator.splits(\n",
    "    (train_set, val_set),\n",
    "    sort_key = lambda sample : len(sample.sen1),\n",
    "    sort = True,\n",
    "    batch_size = 32,\n",
    "    repeat = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_itr))\n",
    "# print(batch.sen1.permute(1,0)[:2])\n",
    "# print(TEXT.vocab.itos[batch.sen1.permute(1,0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for batch in val_itr:\n",
    "#     print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchtext.vocab import GloVe\n",
    "# glove_emb = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 7432\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab size\", len(TEXT.vocab))\n",
    "# print(TEXT.vocab.stoi['carry'], TEXT.vocab.stoi['carries'], TEXT.vocab.stoi['carried'])\n",
    "# print(TEXT.vocab.vectors[84,:5])\n",
    "# print(TEXT.vocab.vectors[780,:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "2018EE10957_A3_A_train.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
