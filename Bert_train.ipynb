{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2018EE10957_A3_B_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWjwfx8T5OAJ"
      },
      "source": [
        "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
        "\n",
        "Give editor access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spAvH1fF0Rhg",
        "outputId": "03adcb8b-10a2-40ef-b5e4-2b5173f8409c"
      },
      "source": [
        "## DONT CHANGE THIS CELL\n",
        "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
        "!unzip data.zip\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-25 08:43:06--  http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Resolving www.cse.iitd.ac.in (www.cse.iitd.ac.in)... 103.27.9.152\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip [following]\n",
            "--2021-10-25 08:43:06--  https://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/data.zip\n",
            "Connecting to www.cse.iitd.ac.in (www.cse.iitd.ac.in)|103.27.9.152|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 217313 (212K) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>] 212.22K   116KB/s    in 1.8s    \n",
            "\n",
            "2021-10-25 08:43:09 (116 KB/s) - ‘data.zip’ saved [217313/217313]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "   creating: data/train/\n",
            "  inflating: data/train/train.gold.txt  \n",
            "  inflating: data/train/train.data.txt  \n",
            "   creating: data/validation/\n",
            "  inflating: data/validation/validation.data.txt  \n",
            "  inflating: data/validation/validation.gold.txt  \n",
            "Collecting transformers\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 41.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 36.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting huggingface-hub>=0.0.17\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.19 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.11.3\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.14.0-py3-none-any.whl (290 kB)\n",
            "\u001b[K     |████████████████████████████████| 290 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: huggingface-hub<0.1.0,>=0.0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.0.19)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 47.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2021.10.1-py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 48.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0,>=0.0.19->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 44.8 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 48.8 MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, yarl, async-timeout, fsspec, aiohttp, xxhash, datasets\n",
            "Successfully installed aiohttp-3.7.4.post0 async-timeout-3.0.1 datasets-1.14.0 fsspec-2021.10.1 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrnkLN2LzlDB"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import datetime\n",
        "import numpy as np\n",
        "import pandas\n",
        "from pdb import set_trace\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_metric"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sr3ok6g51O_d"
      },
      "source": [
        "## Various utility functions\n",
        "class args_class:\n",
        "    dataset = \"data/\"\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    gpu = 0\n",
        "\n",
        "def set_seed(seed=4):\n",
        "\t# fixing seeds to reproduce results (exact!)\n",
        "\ttorch.backends.cudnn.deterministic = True\n",
        "\ttorch.manual_seed(seed)\n",
        "\trandom.seed(seed)\n",
        "\tnp.random.seed(seed)\n",
        "\n",
        "def parse_args():\n",
        "\treturn args_class()\n",
        "\n",
        "def get_device(gpu_no):\n",
        "\tif torch.cuda.is_available():\n",
        "\t\ttorch.cuda.set_device(gpu_no)\n",
        "\t\treturn torch.device('cuda:{}'.format(gpu_no))\n",
        "\telse:\n",
        "\t\treturn torch.device('cpu')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14EjR4tmz2x5"
      },
      "source": [
        "def process_data(path, train=True, test=False):\n",
        "    if test == True:\n",
        "        x_test = pandas.read_csv(\"test.data.txt\", delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
        "        word1 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
        "        word2 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
        "        x_test = x_test.drop(\"position\", axis=1)\n",
        "        x_test.loc[:, \"POS\"] = x_test.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
        "        x_test.insert(3, \"word1\", word1)\n",
        "        x_test.insert(5, \"word2\", word2)\n",
        "        # x_test.to_csv(\"./test.csv\", header=None, index=False)\n",
        "        return x_test\n",
        "    \n",
        "    train = \"train\" if train else \"validation\"\n",
        "    x_train = pandas.read_csv(os.path.join(path, \"%s/%s.data.txt\"%(train, train)), delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
        "    y_train = pandas.read_csv(os.path.join(path, \"%s/%s.gold.txt\"%(train, train)), header=None, names=['label'])\n",
        "    dataset = pandas.concat((x_train, y_train), axis=1)\n",
        "    word1 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
        "    word2 = dataset.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
        "    dataset = dataset.drop(\"position\", axis=1)\n",
        "    dataset.loc[:, \"POS\"] = dataset.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
        "    dataset.loc[:, \"label\"] = dataset.loc[:, \"label\"].apply(lambda lab : 0 if lab==\"F\" else 1)\n",
        "    dataset.insert(3, \"word1\", word1)\n",
        "    dataset.insert(5, \"word2\", word2)\n",
        "    # dataset.to_csv(os.path.join(path, \"%s/%s.csv\"%(train, train)), header=None, index=False)\n",
        "    return dataset\n",
        "\n",
        "class myDataset(Dataset):\n",
        "    def __init__(self, data_dir, max_len=128, train=True, test=False, model_name='bert-base-uncased'):\n",
        "        self.data_dir = data_dir\n",
        "        self.max_len = max_len\n",
        "        self.train = train\n",
        "        self.test = test\n",
        "\n",
        "        # convert raw data into pandas data frame\n",
        "        self.dataset_df = process_data(data_dir, train=train, test=test)\n",
        "\n",
        "        # tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name) # , do_lower_case=True)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.dataset_df.iloc[index]\n",
        "\n",
        "        x = self.tokenizer(sample['sen1'], sample['sen2'], return_tensors='pt', add_special_tokens=True, \\\n",
        "            max_length=self.max_len, padding='max_length', truncation=True)\n",
        "        if self.test == False:\n",
        "            y = torch.tensor(sample['label'])\n",
        "\n",
        "        # format as required by Train\n",
        "        item = {k: v[0] for k,v in x.items()} # only one datapoint; shape: 128\n",
        "        if self.test == False:\n",
        "            item['labels'] = y\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.dataset_df)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rl5jiYUuTzBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed2305ae-c5ae-4f8c-b03c-6439112538aa"
      },
      "source": [
        "set_seed()\n",
        "\n",
        "metric = load_metric(\"accuracy\")\n",
        "def compute_metrics_fn(eval_prediction):\n",
        "    logits, labels = eval_prediction\n",
        "    predictions = logits.argmax(axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "args = parse_args()\n",
        "trainset = myDataset(args.dataset, max_len=100, train=True, model_name=args.model_name)\n",
        "devset = myDataset(args.dataset, max_len=100, train=False, model_name=args.model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(args.model_name, num_labels=2)\n",
        "print(\"resource preparation done: {}\".format(datetime.datetime.now()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/45c3f7a79a80e1cf0a489e5c62b43f173c15db47864303a55d623bb3c96f72a5.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/534479488c54aeaf9c3406f647aa2ec13648c06771ffe269edabebd4c412da1d.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
            "loading file https://huggingface.co/bert-base-uncased/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/c1d7f0a763fb63861cc08553866f1fc3e5a6f4f07621be277452d26d71303b7e.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading configuration file https://huggingface.co/bert-base-uncased/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/3c61d016573b14f7f008c02c4e51a366c67ab274726fe2910691e2a761acf43e.37395cee442ab11005bcd270f3c34464dc1704b715b5d7d52b1a461abe3b9e4e\n",
            "Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.11.3\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "loading weights file https://huggingface.co/bert-base-uncased/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/a8041bf617d7f94ea26d15e218abd04afc2004805632abc0ed2066aa16d50d04.faf6ea826ae9c5867d12b22257f9877e6b8367890837bd60f7c54a29633f7f2f\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "resource preparation done: 2021-10-25 09:31:11.851447\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Lrl1BDMaTy5T",
        "outputId": "e2076b22-7957-4fc2-d0a9-6c83c2c4bfe9"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"2018EE10957_B_model\", \n",
        "    overwrite_output_dir=True, \n",
        "    per_device_eval_batch_size=16,\n",
        "    per_device_train_batch_size=16,\n",
        "    evaluation_strategy=\"epoch\", \n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    num_train_epochs=5,\n",
        "    seed=4,\n",
        "    save_strategy=\"epoch\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_accuracy\",\n",
        "    greater_is_better=True,\n",
        "    warmup_steps=500,\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy=\"epoch\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model, args=training_args,\n",
        "    train_dataset=trainset, eval_dataset=devset,\n",
        "    compute_metrics=compute_metrics_fn\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "***** Running training *****\n",
            "  Num examples = 5428\n",
            "  Num Epochs = 5\n",
            "  Instantaneous batch size per device = 16\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 1700\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1700' max='1700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1700/1700 16:57, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.626100</td>\n",
              "      <td>0.667261</td>\n",
              "      <td>0.656740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.434600</td>\n",
              "      <td>0.644094</td>\n",
              "      <td>0.697492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.233400</td>\n",
              "      <td>1.012652</td>\n",
              "      <td>0.691223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.098700</td>\n",
              "      <td>1.481145</td>\n",
              "      <td>0.694357</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.038700</td>\n",
              "      <td>1.602394</td>\n",
              "      <td>0.710031</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running Evaluation *****\n",
            "  Num examples = 638\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to 2018EE10957_B_model/checkpoint-340\n",
            "Configuration saved in 2018EE10957_B_model/checkpoint-340/config.json\n",
            "Model weights saved in 2018EE10957_B_model/checkpoint-340/pytorch_model.bin\n",
            "Deleting older checkpoint [2018EE10957_B_model/checkpoint-680] due to args.save_total_limit\n",
            "Deleting older checkpoint [2018EE10957_B_model/checkpoint-850] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 638\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to 2018EE10957_B_model/checkpoint-680\n",
            "Configuration saved in 2018EE10957_B_model/checkpoint-680/config.json\n",
            "Model weights saved in 2018EE10957_B_model/checkpoint-680/pytorch_model.bin\n",
            "Deleting older checkpoint [2018EE10957_B_model/checkpoint-340] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 638\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to 2018EE10957_B_model/checkpoint-1020\n",
            "Configuration saved in 2018EE10957_B_model/checkpoint-1020/config.json\n",
            "Model weights saved in 2018EE10957_B_model/checkpoint-1020/pytorch_model.bin\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 638\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to 2018EE10957_B_model/checkpoint-1360\n",
            "Configuration saved in 2018EE10957_B_model/checkpoint-1360/config.json\n",
            "Model weights saved in 2018EE10957_B_model/checkpoint-1360/pytorch_model.bin\n",
            "Deleting older checkpoint [2018EE10957_B_model/checkpoint-1020] due to args.save_total_limit\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 638\n",
            "  Batch size = 16\n",
            "Saving model checkpoint to 2018EE10957_B_model/checkpoint-1700\n",
            "Configuration saved in 2018EE10957_B_model/checkpoint-1700/config.json\n",
            "Model weights saved in 2018EE10957_B_model/checkpoint-1700/pytorch_model.bin\n",
            "Deleting older checkpoint [2018EE10957_B_model/checkpoint-680] due to args.save_total_limit\n",
            "Deleting older checkpoint [2018EE10957_B_model/checkpoint-1360] due to args.save_total_limit\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from 2018EE10957_B_model/checkpoint-1700 (score: 0.7100313479623824).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1700, training_loss=0.2863165973214542, metrics={'train_runtime': 1018.6079, 'train_samples_per_second': 26.644, 'train_steps_per_second': 1.669, 'total_flos': 1394694148920000.0, 'train_loss': 0.2863165973214542, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxKdCr2jUCwP",
        "outputId": "d2540a8e-cadd-4d1e-91d4-c815ea470a6c"
      },
      "source": [
        "## Zip the final model and all the required files, such as vocabulary\n",
        "# Replace USERID with your own, such as 2017CSZ8058\n",
        "!zip -r 2018EE10957_B_model.zip 2018EE10957_B_model\n",
        "\n",
        "## Upload it to Google drive and ensure that the testing notebook uses the correct link"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: 2018EE10957_B_model/ (stored 0%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/ (stored 0%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/rng_state.pth (deflated 27%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/training_args.bin (deflated 48%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/optimizer.pt (deflated 24%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/config.json (deflated 49%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/scheduler.pt (deflated 50%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/pytorch_model.bin (deflated 7%)\n",
            "  adding: 2018EE10957_B_model/checkpoint-1700/trainer_state.json (deflated 73%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mn6wE-LU_Bc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJjx9_YFhRzA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3ccWcHDhXfV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}