{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JSkHozvq7gGh"
   },
   "source": [
    "Make a copy of this notebook and rename using your USERID in the following format, 2017CSZ8058\n",
    "\n",
    "Give read access to keshavkolluru@gmail.com, vishalsaley114@gmail.com and kartikeya.badola@gmail.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sPmKrdbC48JU"
   },
   "outputs": [],
   "source": [
    "# ## DONT CHANGE THIS CELL \n",
    "# # this is currently the same as dev.data.txt\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.data.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xJv-12oi_zKg"
   },
   "outputs": [],
   "source": [
    "## Replace with the right link that contains the zip file uploaded from the training\n",
    "!gdown https://drive.google.com/uc?id=xxxxxxxxxxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9Sh-Mw6cAIcH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import torch\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = pandas.read_csv(\"data/test.data.txt\", delimiter='\\t', header=None, names=['word', 'POS', 'position', 'sen1', 'sen2'])\n",
    "word1 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[0])\n",
    "word2 = x_test.loc[:,'position'].apply(lambda positions : positions.split(\"-\")[1])\n",
    "x_test = x_test.drop(\"position\", axis=1)\n",
    "x_test.loc[:, \"POS\"] = x_test.loc[:, \"POS\"].apply(lambda POS_TAG : 0 if POS_TAG==\"V\" else 1)\n",
    "x_test.insert(3, \"word1\", word1)\n",
    "x_test.insert(5, \"word2\", word2)\n",
    "x_test.to_csv(\"data/test.csv\", header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "with open(os.path.join(\"data\", \"Field_TEXT\"), 'rb') as f:\n",
    "    TEXT = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7462, 100])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "    ('word', TEXT),\n",
    "    ('POS', data.Field(use_vocab=False, sequential=False)), \n",
    "    ('sen1', TEXT),\n",
    "    ('word1', data.Field(use_vocab=False, sequential=False)),\n",
    "    ('sen2', TEXT),\n",
    "    ('word2', data.Field(use_vocab=False, sequential=False)),\n",
    "]\n",
    "\n",
    "test_set = data.TabularDataset.splits(\n",
    "    path = \"data\",\n",
    "    test = \"test.csv\",\n",
    "    format = 'csv',\n",
    "    fields = fields,\n",
    "    skip_header = False\n",
    ")\n",
    "\n",
    "(test_itr,) = data.BucketIterator.splits(\n",
    "    test_set,\n",
    "    #sort_key = lambda sample : len(sample.sen1),\n",
    "    sort = False,\n",
    "    batch_size = 32,\n",
    "    repeat = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.legacy.data.iterator.BucketIterator at 0x7fa119198c70>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_itr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pandas\n",
    "import logging\n",
    "from argparse import ArgumentParser\n",
    "from pdb import set_trace\n",
    "import torch\n",
    "import torch.optim as O\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torchtext.legacy import data\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class myModel(nn.Module):\n",
    "    def __init__(self, pretrained_embeddings, embed_dim=100, hidden_dim=100, device=torch.device('cpu')):\n",
    "        super(myModel, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.dropout = nn.Dropout(p = 0.5)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.similarity = nn.CosineSimilarity(dim=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        '''\n",
    "            [torchtext.legacy.data.batch.Batch of size 32]\n",
    "                [.word]:[torch.LongTensor of size 1x32]\n",
    "                [.POS]:[torch.LongTensor of size 32]\n",
    "                [.sen1]:[torch.LongTensor of size 17x32]\n",
    "                [.word1]:[torch.LongTensor of size 32]\n",
    "                [.sen2]:[torch.LongTensor of size 26x32]\n",
    "                [.word2]:[torch.LongTensor of size 32]\n",
    "                [.label]:[torch.LongTensor of size 32]\n",
    "        '''\n",
    "        input1 = input.sen1.permute(1,0).to(self.device)\n",
    "        input2 = input.sen2.permute(1,0).to(self.device)\n",
    "\n",
    "        # embed the tokens (indices in the vocab) \n",
    "        out1 = self.dropout(self.embedding(input1))\n",
    "        out2 = self.dropout(self.embedding(input2))\n",
    "\n",
    "        out1, (_, _) = self.bilstm(out1)\n",
    "        out2, (_, _) = self.bilstm(out2)\n",
    "\n",
    "        # word of interest\n",
    "        woi1_idx = torch.repeat_interleave(torch.unsqueeze(torch.unsqueeze(input.word1, 1), 2), 2*self.hidden_dim, dim=2).to(self.device)\n",
    "        woi2_idx = torch.repeat_interleave(torch.unsqueeze(torch.unsqueeze(input.word2, 1), 2), 2*self.hidden_dim, dim=2).to(self.device)\n",
    "\n",
    "        # gather the word of interest for each sentence in the batch\n",
    "        out1 = torch.gather(input=out1, dim=1, index=woi1_idx).squeeze()\n",
    "        out2 = torch.gather(input=out2, dim=1, index=woi2_idx).squeeze()\n",
    "        \n",
    "        # compute scores of similarity\n",
    "        out = self.similarity(out1, out2)\n",
    "        out = self.sigmoid(out)\n",
    "        # try:\n",
    "        #     assert out.shape[0] == self.bs\n",
    "        # except:\n",
    "        #     print(\"AssertionError\", out.shape[0], self.bs) # all samples are touched in a epoch\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myModel(pretrained_embeddings=TEXT.vocab.vectors, embed_dim=TEXT.vocab.vectors.shape[1], hidden_dim=100, device=torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.load(\"data/best-bilstm-params.pt\",map_location=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(x['model_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T\\n', 'F\\n', 'T\\n']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('output.txt', 'r').readlines()\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-IlAUkv7s1C"
   },
   "outputs": [],
   "source": [
    "## DONT CHANGE THIS CELL\n",
    "# Your testing code must produce a file output.txt with predictions as T and F in each line\n",
    "\n",
    "## Final Evaluation \n",
    "# this is currently the same as dev.gold.txt\n",
    "!wget http://www.cse.iitd.ac.in/~mausam/courses/col772/autumn2021/A3/test.gold.txt\n",
    "correct, total = 0., 0.\n",
    "for prediction, gold in zip(open('output.txt'), open('test.gold.txt')):\n",
    "    prediciton, gold = prediction.strip(), gold.strip()\n",
    "    total += 1\n",
    "    if prediction == gold:\n",
    "        correct += 1\n",
    "\n",
    "## Report this as the final validation performance \n",
    "print('Performance = '%(correct/total))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2018EE10957_A3_A_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
